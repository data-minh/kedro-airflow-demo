{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3.9'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3.9'\n",
    "\n",
    "# Set HADOOP_CONF_DIR environment variable\n",
    "os.environ['HADOOP_CONF_DIR'] = '/usr/odp/1.2.4.0-102/hadoop/etc/hadoop'\n",
    "\n",
    "# Set ARROW_LIBHDFS_DIR environment variable\n",
    "os.environ['ARROW_LIBHDFS_DIR'] = '/usr/odp/1.2.4.0-102/hadoop/lib/native/'\n",
    "\n",
    "# Set CLASSPATH enviroment variable\n",
    "classpath = subprocess.check_output(['hadoop', 'classpath', '--glob'])\n",
    "os.environ['CLASSPATH'] = classpath.decode('utf-8')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/20 15:16:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/20 15:16:15 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "25/06/20 15:16:16 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "ICEBERG_VERSION = \"1.4.3\" \n",
    "ICEBERG_SPARK_VERSION = \"3.5\" \n",
    "ICEBERG_SCALA_VERSION = \"2.12\" \n",
    "ICEBERG_EXTENSION_CLASS = \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\" \n",
    "ICEBERG_SPARK_SQL_CATALOG_NAME = \"org.apache.iceberg.spark.SparkCatalog\" \n",
    "ICEBERG_PATH_JAR = f\"/opt/spark-3.5.1-bin-hadoop3/jars/iceberg-spark-runtime-{ICEBERG_SPARK_VERSION}_{ICEBERG_SCALA_VERSION}-{ICEBERG_VERSION}.jar\"\n",
    "ICEBERG_HIVE_CATALOG_NAME = 'curated'\n",
    "THRIFT_IP = '10.53.2.116' \n",
    "THRIFT_PORT = 9083 \n",
    "HIVE_POSTGRES_PATH_JAR = f'/opt/spark-3.5.1-bin-hadoop3/jars/postgresql-42.2.25.jar' \n",
    "THRIFT_HIVE_URI = f'thrift://{THRIFT_IP}:{THRIFT_PORT}'\n",
    "SPARK_JARS_ADDITION = \",\".join([ \n",
    "    ICEBERG_PATH_JAR, \n",
    "    HIVE_POSTGRES_PATH_JAR,\n",
    "    '/home/VT_TTDLPT_MINHPN5/lib_spark/spark-avro_2.12-3.5.1.jar'\n",
    "]) \n",
    "\n",
    "spark: SparkSession = (\n",
    "    SparkSession.builder.appName(\"kedro_dev\")\n",
    "        .config(\"spark.master\", \"yarn\")\n",
    "        .config(\"spark.deploy.mode\", \"cluster\")\n",
    "        .config(\"spark.driver.cores\", \"1\")\n",
    "        .config(\"spark.driver.memory\", \"1g\")\n",
    "        .config(\"spark.executor.cores\", \"1\")\n",
    "        .config(\"spark.executor.memory\", \"1g\")\n",
    "        .config(\"spark.executor.instances\", \"2\")\n",
    "        .config(\"spark.executor.memoryOverhead\", \"512m\")\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "        .config(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "        .config(\"spark.sql.parquet.int96RebaseModeInRead\", \"CORRECTED\")\n",
    "        .config(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
    "        .config(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n",
    "        .config(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "        .config(\"spark.jars\", SPARK_JARS_ADDITION)\n",
    "        .config(\"spark.sql.extensions\", ICEBERG_EXTENSION_CLASS)\n",
    "        .config(f\"spark.sql.catalog.{ICEBERG_HIVE_CATALOG_NAME}\", ICEBERG_SPARK_SQL_CATALOG_NAME)\n",
    "        .config(f\"spark.sql.catalog.{ICEBERG_HIVE_CATALOG_NAME}.type\", \"hive\")\n",
    "        .config(f\"spark.sql.catalog.{ICEBERG_HIVE_CATALOG_NAME}.uri\", THRIFT_HIVE_URI)\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|      namespace|\n",
      "+---------------+\n",
      "|    biss_mviews|\n",
      "|        default|\n",
      "|          hive1|\n",
      "|      hive2_csv|\n",
      "|     hive2_csv4|\n",
      "|   iceberg_bcvm|\n",
      "|iceberg_feature|\n",
      "|iceberg_staging|\n",
      "|  nzdr_20241012|\n",
      "|         ttqtdl|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show namespaces in curated\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------------------------------------------------------------------------------+\n",
      "|info_name     |info_value                                                                                    |\n",
      "+--------------+----------------------------------------------------------------------------------------------+\n",
      "|Catalog Name  |curated                                                                                       |\n",
      "|Namespace Name|kedro_dev                                                                                     |\n",
      "|Comment       |Namespace for business intelligence and analytics marts                                       |\n",
      "|Location      |hdfs://10.53.2.40:8020/user/VT_TTDLPT_MINHPN5/iceberg/kedro_dev                               |\n",
      "|Owner         |VT_TTDLPT_MINHPN5                                                                             |\n",
      "|Properties    |((hive.metastore.database.owner,VT_TTDLPT_MINHPN5), (hive.metastore.database.owner-type,USER))|\n",
      "+--------------+----------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE NAMESPACE EXTENDED curated.kedro_dev\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
