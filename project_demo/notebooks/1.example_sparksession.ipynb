{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3.9'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3.9'\n",
    "\n",
    "# Set HADOOP_CONF_DIR environment variable\n",
    "os.environ['HADOOP_CONF_DIR'] = '/usr/odp/1.2.4.0-102/hadoop/etc/hadoop'\n",
    "\n",
    "# Set ARROW_LIBHDFS_DIR environment variable\n",
    "os.environ['ARROW_LIBHDFS_DIR'] = '/usr/odp/1.2.4.0-102/hadoop/lib/native/'\n",
    "\n",
    "# Set CLASSPATH enviroment variable\n",
    "classpath = subprocess.check_output(['hadoop', 'classpath', '--glob'])\n",
    "os.environ['CLASSPATH'] = classpath.decode('utf-8')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/20 14:02:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/06/20 14:02:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/06/20 14:02:56 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/06/20 14:02:56 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/06/20 14:02:56 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/06/20 14:02:56 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/06/20 14:02:57 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "25/06/20 14:02:57 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"kedro_dev\")\n",
    "        .config(\"spark.master\", \"yarn\")\n",
    "        .config(\"spark.deploy.mode\", \"cluster\")\n",
    "        .config(\"spark.driver.cores\", \"1\")\n",
    "        .config(\"spark.driver.memory\", \"1g\")\n",
    "        .config(\"spark.executor.cores\", \"1\")\n",
    "        .config(\"spark.executor.memory\", \"1g\")\n",
    "        .config(\"spark.executor.instances\", \"2\")\n",
    "        .config(\"spark.executor.memoryOverhead\", \"512m\")\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "        .config(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "        .config(\"spark.sql.parquet.int96RebaseModeInRead\", \"CORRECTED\")\n",
    "        .config(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
    "        .config(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n",
    "        .config(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/VT_TTDLPT_MINHPN5/kedro-dev/data/01_raw/raw.csv')\n",
    "\n",
    "# Hiển thị 5 dòng đầu tiên\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "spark_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"hdfs://10.53.2.40:8020/user/VT_TTDLPT_MINHPN5/test/raw/2025-01-07\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('/home/VT_TTDLPT_MINHPN5/kedro-dev/data/01_raw/intermediate_information.csv')\n",
    "\n",
    "# Hiển thị 5 dòng đầu tiên\n",
    "spark_df_1 = spark.createDataFrame(df_1)\n",
    "\n",
    "spark_df_1.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"processed_date\") \\\n",
    "    .parquet(\"hdfs://10.53.2.40:8020/user/VT_TTDLPT_MINHPN5/test/intermediate_information\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------+\n",
      "| id|                name|processed_date|\n",
      "+---+--------------------+--------------+\n",
      "|  6|MinhfLKFNACN JDAS...|    2025-01-06|\n",
      "|  6|MinhfLKFNACN JDAS...|    2025-01-06|\n",
      "|  7|aslkjBFNJA DNJHVS...|    2025-01-07|\n",
      "|  7|aslkjBFNJA DNJHVS...|    2025-01-07|\n",
      "+---+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df_reloaded = spark.read.parquet(\n",
    "    \"hdfs://10.53.2.40:8020/user/VT_TTDLPT_MINHPN5/test/feature_information\"\n",
    ")\n",
    "\n",
    "spark_df_reloaded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_reloaded = spark.read.parquet(\n",
    "    \"hdfs://10.53.2.40:8020/user/VT_TTDLPT_MINHPN5/test/raw/{2025-01-06,2025-01-07}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------+\n",
      "| id|                name|processed_date|\n",
      "+---+--------------------+--------------+\n",
      "|  7|ssdvbskadbvkasahd...|    2025-01-07|\n",
      "|  6|sasahdadasjbdasjdhdv|    2025-01-06|\n",
      "+---+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df_reloaded.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
